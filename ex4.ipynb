{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:08:40.560718Z",
     "start_time": "2025-12-15T14:08:40.549615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n"
   ],
   "id": "5f250791af3ce614",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:08:40.652897Z",
     "start_time": "2025-12-15T14:08:40.636858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, year=\"2007\", image_set=\"train\", transforms=None):\n",
    "        self.dataset = VOCDetection(root, year=year, image_set=image_set, download=False)\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.dataset[idx]\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in target['annotation']['object']:\n",
    "            bbox = obj['bndbox']\n",
    "            boxes.append([\n",
    "            float(bbox['xmin']),\n",
    "            float(bbox['ymin']),\n",
    "            float(bbox['xmax']),\n",
    "            float(bbox['ymax'])\n",
    "        ])\n",
    "            labels.append(1) # single-class example\n",
    "\n",
    "\n",
    "        target = {\n",
    "        \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ],
   "id": "30a7f3593c40f4e0",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T15:40:08.967484Z",
     "start_time": "2025-12-15T15:40:08.849596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "train_dataset = VOCDataset(\"./data\", image_set=\"train\")\n",
    "val_dataset = VOCDataset(\"./data\", image_set=\"val\")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(train_dataset, range(100))\n",
    "val_dataset = Subset(val_dataset, range(50))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "adcb25d5b84740fb",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:08:42.563972Z",
     "start_time": "2025-12-15T14:08:40.954695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "num_classes = 21  # 20 VOC classes + background\n",
    "\n",
    "# Φτιάχνουμε το pretrained μοντέλο\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Αντικαθιστούμε το box predictor\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes\n",
    ")\n",
    "\n",
    "# Στέλνουμε όλο το μοντέλο στο σωστό device\n",
    "model = model.to(device)"
   ],
   "id": "cab079990dec1b54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:58:03.285669Z",
     "start_time": "2025-12-15T14:08:42.616377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005\n",
    ")\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # tqdm wrapper για τον train_loader\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss:.2f}\")\n"
   ],
   "id": "c197c37173cd475c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 50/50 [10:05<00:00, 12.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 29.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 50/50 [10:11<00:00, 12.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 13.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 50/50 [08:54<00:00, 10.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 10.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 50/50 [10:20<00:00, 12.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 9.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 50/50 [09:48<00:00, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 7.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005\n",
    ")\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # tqdm wrapper για τον train_loader\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss:.2f}\")\n"
   ],
   "id": "dce126cbe18092d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005\n",
    ")\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # tqdm wrapper για τον train_loader\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss:.2f}\")\n"
   ],
   "id": "9743aef5d9d6ec7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
